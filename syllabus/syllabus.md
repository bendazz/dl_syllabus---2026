# Fundamentals of Deep Learning

## Course Information

- Course: CIS-171-01FTM: FUNDAMENTALS OF DEEP LRNG
- Semester: Spring 2026
- Location: Library 126
- Schedule: MWF 9:00 AM - 9:50 AM
- Credits: 3

## Instructor Information

- Instructor: Dr. Chad Redmond
- Office: Library 416
- Email: credmond@mercyhurst.edu
- Phone: (814) 969-2269

## Office Hours

- Daytime: MWF 11:00 AM – 12:00 PM (Library 416)
- Evening: TW 8:15 PM – 9:45 PM (Cyber Labs)

## Course Description

This course introduces the fundamentals of deep learning through three focused modules: (1) basic feedforward neural networks, (2) convolutional neural networks (CNNs), and (3) transformer architectures. Across all modules, we emphasize backpropagation, optimization, and practical model development using PyTorch’s `autograd` and `nn` APIs. Students will build interactive Gradio applications that showcase model behavior, enabling rapid experimentation, user-friendly demos, and clear communication of results. By the end of the course, you will be able to implement, train, evaluate, and deploy core deep learning models and present them as usable web apps.

## Learning Objectives

By the end of this course, you will be able to:

- Distinguish among feedforward networks, convolutional neural networks, and transformer architectures, and select an appropriate model for a given task.
- Implement and train these models in PyTorch using `autograd` and the `nn` module with clear training/validation loops.
- Explain and apply backpropagation; diagnose gradient issues; and improve training with initialization, normalization, dropout, and weight decay.
- Prepare data pipelines with `Dataset`/`DataLoader`; apply basic preprocessing and augmentation for images and text.
- Evaluate models with suitable metrics and validation strategies; perform basic hyperparameter tuning, learning-rate scheduling, and early stopping.
- Build simple Gradio applications that wrap trained models for interactive inference and classroom-ready demos.


## Grading

- Exams: Three tests and one final exam, each worth 100 points (total 400 points). All exams are on paper; closed-book/closed-notes; no devices permitted.
- Letter grade lower boundaries (by final percentage over 400 points):
	- A: 90%
	- B+: 80%
	- B: 70%
	- C+: 65%
	- C: 60%
	- D+: 55%
	- D: 50%
	- F: 0%
- Attendance: Every 10 absences results in a one-letter grade reduction applied after computing the letter grade from points.

## Recommended References

- Hands-On Machine Learning with Scikit-Learn and PyTorch — Sebastian Raschka
- Build a Large Language Model (From Scratch) — Sebastian Raschka



## Tentative Schedule (15 Weeks)

- Week 1: PyTorch and Tensors.
- Week 2: Review of linear regression.
- Week 3: Review of Logistic Regression.
- Week 4: Review and Test 1.
- Week 5: Backpropagation.
- Week 6: Regularization, Dropout, Learning Rate Scheduling.
- Week 7: Convolutional Neural Networks.
- Week 8: Convolutional Neural Networks (continued).
- Week 9: Review and Test 2.
- Week 10: Gradio Apps.
- Week 11: Tokenizing text.
-- Week 12: Token Embeddings.
-- Week 13: Review and Test 4.
-- Week 14: Attention Mechanisms.
 - Week 15: Attention Mechanisms.

Final Exam: Scheduled during finals week (date/time TBA by the registrar).

